# DQL_TicTacToe_20x20

Описание среды:
 Размер поля: 20x20 клеток.
 Игроки: Два игрока — один управляется искусственным интеллектом
(агент), другой может быть человеком или другим ИИ.
 Цель игры: Поставить в ряд пять своих символов (крестики или нолики) по
горизонтали, вертикали или диагонали.
Описание состояния:
 Состояние: Представляется как текущее расположение всех крестиков и
ноликов на поле 20x20.
 Представление состояния: Используется двумерный массив размера 20x20,
где:
o Пустая клетка — 0,
o Крестик — 1,
o Нолик — -1.
 Пространство состояний: В общей сложности 3
400 состояний, что делает
использование Q-таблицы невозможным.
Описание действий:
 Действие: Выбор клетки для размещения своего символа.
 Возможные действия: От 0 до 399 (индекс клетки в матрице 20x20).
 Ограничения: Действие допустимо только если клетка свободна (содержит
0).
Описание системы наград:
 Победа агента: +10
 Поражение агента: -5
 Ничья: -1
 Неправильное действие (попытка поставить символ в занятую клетку): -10.
Описание алгоритма обучения:
 Q-функция: Аппроксимируется с помощью нейронной сети.
 Архитектура сети DNetwork: Входной слой принимает состояние размером
1200 несколько скрытых слоев с активацией ReLU, выходной слой с 400
нейронами для каждого возможного действия.
𝑖𝑛𝑝𝑢𝑡(1200) −> 𝑓𝑐1(1024) −> 𝑅𝑒𝐿𝑈 −> 𝑓𝑐2(512) −> 𝑅𝑒𝐿𝑈 −
> 𝑓𝑐3(256) −> 𝑅𝑒𝐿𝑈 −> 𝑜𝑢𝑡𝑝𝑢𝑡(400)
 Обновление Q-функции:
𝑄(𝑠, 𝑎) ← 𝑄(𝑠, 𝑎) + 𝛼[𝑟 + 𝛾𝑚𝑎𝑥𝑎′𝑄(𝑠′, 𝑎′) − 𝑄(𝑠, 𝑎)]
где:
o α— скорость обучения,
o γ — коэффициент дисконтирования,
o r — полученная награда,
o s′ — новое состояние после выполнения действия a.
Описание процесса симуляции игры:
1. Инициализация: Создание пустого поля.
2. Игровой цикл:
o Проверка текущего состояния игры (победа, ничья, продолжаем
игру).
o Выбор действия агентом на основе Q-значений, аппроксимированных нейронной сетью.
o Выполнение действия и обновление состояния игры.
o Противник выполняет свое действие.
o Повтор до завершения игры (победа одного из игроков или ничья).
3. Обновление нейронной сети после каждого шага с использованием опыта.
Ограничения проекта:
 Большое пространство состояний: Огромное количество возможных состояний требует использования нейронных сетей для аппроксимации Q-функции.
 Баланс обучения и вычислительной сложности: Поддержание баланса
между скоростью обучения и качеством игры агента.
 Эффективное хранение данных: Буфер для хранения опыта для обновления
нейронной сети.
